setwd("C:/miscosas/empresas/notas30012/data")
dFile<-'Taxi_Trips.csv'
chunkSize<-10000000
#con<-file(description=dFile,open="r")
#all_d<-read.table(con,nrows=chunkSize,header=T,fill=TRUE,sep=",")
#close(con)
library(data.table)
all_d<-fread(dFile,nrows=chunkSize,header=T,sep=",")
all_d$Fare<-as.numeric(gsub('\\$', '', all_d$Fare))
all_d$Tips<-as.numeric(gsub('\\$', '', all_d$Tips))
all_d<-as.data.frame(all_d)
who<-which(all_d$Fare<20) 
all_d<-all_d[-who,]
who<-which(all_d$Fare>200)
all_d<-all_d[-who,]


#tips.rat<- all_d$Tips/ all_d$Fare #thing is that in most cases either there are no tips or about 15/20% (w few exceptions that ppl give more)
#all_d$tips.rat<-tips.rat
check <-as.POSIXct(strptime(format(all_d[,4], format="%x %X"),"%m/%d/%Y %I:%M:%S %p",tz=""))
all_d$day <- as.factor(weekdays(check))
library(lubridate)
all_d$timeint<-cut(hour(check), breaks = c(0, 6, 12, 18, 24), include.lowest = TRUE) 


summary(as.factor(unlist(all_d[,16])))

who<-which(all_d[,16]=="Unknown")
all_d<-all_d[-who,]
who<-which(all_d[,16]=="Dispute"|all_d[,16]=="No Charge")
all_d<-all_d[-who,]

who<-which(all_d[,16]=="Pcard"|all_d[,16]=="Prcard") #TAP cards removed
all_d<-all_d[-who,]

who<-which(all_d[,16]=="Mobile") #credit card? unknown so removed
all_d<-all_d[-who,]

data.dup <- all_d[duplicated(all_d) | duplicated(all_d, fromLast = TRUE), ]
dim(data.dup) #no duplicates!

all_d[,16]<-as.factor(unlist(all_d[,16]))

all_d2<-all_d[,c(5,6,9,10,11,12,16,25,26)]
names(all_d2)<-c("Duration","Distance","Pickup.loc","Dropoff.loc","Fare","Tips","Pay.type","Day","Tod")
names(all_d2)
all_d2$Pickup.loc<-as.factor(all_d2$Pickup.loc)
all_d2$Dropoff.loc<-as.factor(all_d2$Dropoff.loc)
who<-which(all_d2$Distance<5|all_d2$Distance>30)
all_d2<-all_d2[-who,]
who<-which(all_d2$Duration<300|all_d2$Duration>7200)
all_d2<-all_d2[-who,]
speed<-(all_d2$Distance/all_d2$Duration)*3600
who<-which(speed>70) #lowest spped is not too slow
all_d2<-all_d2[-who,]
NROW(na.omit(all_d2)) #number of complete cases that will be used for the model
all_d2<-all_d2[complete.cases(all_d2), ]

d10k<-all_d2[1:10000,]
pairs(d10k[,c("Fare","Duration","Distance")],upper.panel=NULL)
cor(d10k[,c("Fare","Duration","Distance")],use="complete.obs")

aggregate(all_d2$Fare,list(all_d2$Tod),length)
aggregate(all_d2$Fare,list(all_d2$Tod),mean)
aggregate(all_d2$Fare,list(all_d2$Tod),sd)

aggregate(Distance~Tod*Day,data=all_d2,mean)

#fit<-lm(Tips~Fare+Trip.time+Trip.miles+Pickup.loc*pay.type+Dropoff.loc*tod+dow*tod,data=all_d2) # ADD taxi company?B  

fit_fare<-lm(Fare~Duration+Distance+Tod+Day+Pickup.loc+Pay.type+Dropoff.loc,data=all_d2)
fit_fare2<-lm(Fare~Duration+Distance*Tod+Distance*Day+Pickup.loc+Pay.type+Dropoff.loc+Day*Tod,data=all_d2)
anova(fit_fare)
anova(fit_fare2)


final_lm<-step(fit_fare)
final_lm2<-step(fit_fare2)

library(glmnet) #function glmnet already standardizes numerical predictors. See Efron and Hastie pg 306
all_d3<-all_d2[1:200000,] #so it runs faster
fit_fare2<-lm(Fare~Duration+Tips+Distance*Tod+Distance*Day+Pickup.loc+Pay.type+Dropoff.loc+Day*Tod,data=all_d3)
xt<-model.matrix(fit_fare2)
yt=all_d3[,5]
cv<-cv.glmnet(x=xt,y=yt,alpha=1,nlambda=10)#alpha=1 do lasso
plot(cv)
fit_lasso<-glmnet(x=xt,y=yt,alpha=1,lambda=cv$lambda.1se)

aggregate(Tips~Pay.type,data=all_d2,mean)
fit_fare3<-lm(Fare~Duration+Distance,data=all_d2)

fit_fare_l<-lm(log(Fare)~Duration+Distance,data=all_d2)

fit_l2<-update(fit_1,.~.,-tod)
final_lm<-step(fit_l)

fit_ls<-lm(log(Tips+1)~Fare+pay.type,data=all_d2)

library(broom) #fortify since ggplot2 may deprecate the function
library(ggplot2)

#create multiplot function
multiplot <- function(..., plotlist = NULL, file, cols = 1, layout = NULL) {
  require(grid)

  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  if (is.null(layout)) {
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

  if (numPlots == 1) {
    print(plots[[1]])

  } else {
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    for (i in 1:numPlots) {
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
myfortfit<-fortify(fit_fare3)#fit_fare_l did not help
minifortfit<-myfortfit[1:100000,]
# USING RESIDUALS INSTEAD OF STD RESIDUALS FOR ANOVA. I INTRODUCE THE STD RESID IN 
# THE SIMPLE LINEAR REG CHAPTER AND CAN USE .stdresid from fortify then
# Homoscedasticity assumption
p1<-ggplot(data = minifortfit, aes(x = .fitted, y = .stdresid)) +
  geom_hline(yintercept = 0, colour = "firebrick3") +
  geom_point()+ylab('Standardized Residual')+xlab('Fitted Values')+
  ggtitle('Standardized Residual vs Fitted')+theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, size=7),axis.title=element_text(size=7)) 



# Normality assumption
p2<-ggplot(data = minifortfit, aes(sample = .stdresid)) +
  stat_qq() + ggtitle('Normal Probability Plot')+ #qq plot and normal probability plot are not the same thing
  ylab('Sample Quantiles')+xlab('Theoretical Quantiles')+
  geom_abline(colour = "firebrick3")+theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, size=7),axis.title=element_text(size=7))

  ncl<-nclass.Sturges(minifortfit$.resid)
p3<-ggplot(minifortfit, aes(x=.stdresid)) + geom_histogram(bins=ncl)#qplot(.stdresid, data=myfortfit,geom="histogram")
    p3<-p3+ylab("")+xlab('Standardized Residual')
    p3<-p3+ggtitle('Histogram')+theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, size=7),axis.title=element_text(size=7))

multiplot(p2,p3,p1,cols=2)


